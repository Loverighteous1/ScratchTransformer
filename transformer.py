# -*- coding: utf-8 -*-
"""Transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LBZ7r8dgZiVe0yCQSpTskJkNKDTPlwcM
"""

import torch as t

"""## Scaled dot-product attention aka Self attention
Attention is a function defined as:

$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + \text{mask}\right)V$

Step 1: Initialization of the inputs  Q, K, V vectors
"""

# Initialization of the inputs  Q, K, V vectors
"""Q query: the query vector is describes what I am looking for
   K keys: is what I can offer
   V values: is what I offer
   sequence = I am starving I am going
   d_Q = d_K, is the input embedding layer dimension. And it is an hyperparameter
   """
t.manual_seed(13)
seq_len, d_Q, d_K, d_V = 6, 8, 8, 8
# torch.randint returns a tensor filled with random integers generated uniformly
Q = t.rand(seq_len, d_Q)
K = t.rand(seq_len, d_K)
V = t.rand(seq_len, d_V)

"""Step 2: Matrices multiplication of the query and keys"""

q_kt = t.matmul(Q, K.T)

"""Step 3: Scaled by $1/sqrt(d_k)$"""

from math import sqrt
scaled = q_kt / sqrt(d_K)

"""Step 4: Apply mask if in decoder case else in encoder do not apply"""

Mask = t.tril(t.ones((seq_len, seq_len)))
Mask[Mask == 0] = -t.infty
Mask[Mask == 1] = 0

scaled_mask = scaled + Mask

"""Step 5: Apply the softmax to produce the probability distribution of the matrix taken"""

def softmax(x):
  return (t.exp(x).T / (t.sum(t.exp(x), axis = -1))).T
  #This computes the sum of exponential values along the last axis of the array

"""Step 6: Multiplication of the weights by the values vector"""

attention = t.mm(softmax(scaled_mask), V)

"""Function"""



"""### Part 2: Starting from the embeddings and creation of the weights processes.

Embeddings
"""

def word2vect_emb(sentence):
  # Create a sorted dictionnary based on the words of the sentence and associating a value sequence as their values
  sentence_dic = {word:value for value, word in enumerate(sorted(sentence.replace(',', '').split()))}
  # Retrieve the indices
  words_indx = t.tensor([sentence_dic[word] for word in sentence.replace(',', '').split()])
  # word embedding using nn module
  words_emb = t.nn.Embedding(len(sentence.split()), embedding_dim=len(sentence.split()) + 4)(words_indx).detach()
  # detach is to avoid tracking the gradient. Same action as required_grad = False
  return words_emb

X = word2vect_emb("sent ence emb, layer bdkfs nkhjdd, vhs")



len("My, ame".split())

a = b = 2
b

"""Weight matrices creations"""

def QKV_Weights(X):
  emb_dim = X.shape[1]
  #d_K, d_Q and d_V are choosen arbitrary, but d_k and d_Q must be the same for the matrix multiplication
  d_K = d_Q = X.shape[0] * 4
  d_V = d_K # For commodity let's assume d_V = d_K ,  (X.shape[0] +1) * 4
  W_Q = t.nn.Parameter(t.rand(d_Q, emb_dim))
  W_K = t.nn.Parameter(t.rand(d_K, emb_dim))
  W_V = t.nn.Parameter(t.rand(d_V, emb_dim))
  return W_Q, W_K, W_V

W_Q, W_K, W_V = QKV_Weights(X)
print(W_Q.shape, W_K.shape, W_V.shape)

"""Query, Key and Values matrices construction"""

def QKV_matrices(X, W_Q, W_K, W_V):
  Q = W_Q @ X.T
  K = W_K @ X.T
  V = W_V @ X.T
  return Q, K, V

Q, K, V = QKV_matrices(X, W_Q, W_K, W_V)
print(Q.shape, K.shape, V.shape)

"""Attention matrix"""

def softmax(x):
  return (t.exp(x)/ (t.sum(t.exp(x), dim=0)))

def scaleDotProaAt(X, Q, K, V):
  Mask = t.tril(t.ones((Q.shape[0], Q.shape[0])))
  Mask[Mask == 0] = -t.tensor(float('inf'))
  Mask[Mask == 1] = 0
  soft = softmax((t.matmul(Q, K.T) / sqrt(d_K)) + Mask)
  attention = t.mm(soft, V)
  return attention



A = scaleDotProaAt(X, Q, K, V)
A.shape

"""## Class Self-attention"""

class ScaledAttention:

  def word2vect_emb(self,sentence):

    # Create a sorted dictionnary based on the words of the sentence and associating a value sequence as their values
    sentence_dic = {word:value for value, word in enumerate(sorted(sentence.replace(',', '').split()))}
    # Retrieve the indices
    words_indx = t.tensor([sentence_dic[word] for word in sentence.replace(',', '').split()])
    # word embedding using nn module
    words_emb = t.nn.Embedding(len(sentence.split()), embedding_dim=len(sentence.split()) + 4)(words_indx).detach()
    # detach is to avoid tracking the gradient. Same action as required_grad = False
    return words_emb

  def QKV_Weights(self,X):
    emb_dim = X.shape[1]
    #d_K, d_Q and d_V are choosen arbitrary, but d_k and d_Q must be the same for the matrix multiplication
    d_K = d_Q = X.shape[0] * 4
    d_V = d_K # For commodity let's assume d_V = d_K ,  (X.shape[0] +1) * 4
    W_Q = t.nn.Parameter(t.rand(d_Q, emb_dim))
    W_K = t.nn.Parameter(t.rand(d_K, emb_dim))
    W_V = t.nn.Parameter(t.rand(d_V, emb_dim))
    return W_Q, W_K, W_V

  def QKV_matrices(self, X, W_Q, W_K, W_V):
    Q = W_Q @ X.T
    K = W_K @ X.T
    V = W_V @ X.T
    return Q, K, V

  def softmax(self, x):
    return (t.exp(x)/ (t.sum(t.exp(x), dim=0)))

  def scaleDotProaAt(self,X, Q, K, V):

    Mask = t.tril(t.ones((Q.shape[0], Q.shape[0])))
    Mask[Mask == 0] = -t.tensor(float('inf'))
    Mask[Mask == 1] = 0
    soft = self.softmax((t.matmul(Q, K.T) / sqrt(d_K)) + Mask)
    attention = t.mm(soft, V)
    return attention

attention = ScaledAttention()

s = "sent ence emb, layer bdkfs nkhjdd, vhs"

words_emb = attention.word2vect_emb(s)

W_Q, W_K, W_V = attention.QKV_Weights(words_emb)

Q, K, V = attention.QKV_matrices(words_emb,W_Q, W_K, W_V)

A_mat = attention.scaleDotProaAt(words_emb,Q, K, V)
A_mat.shape

"""# Class Cross-attention"""

class CrossAttention(ScaledAttention):
  def __init__(self) -> None:
    super().__init__()

  def QKV_Weights(self, X1, X2):
    emb_dim = X1.shape[1]
    #emb_dim2 = X2.shape[1]
    #d_K, d_Q and d_V are choosen arbitrary, but d_k and d_Q must be the same for the matrix multiplication
    d_Q = X1.shape[0] * 4
    d_K = X2.shape[0] * 4
    d_V = d_K # For commodity let's assume d_V = d_K ,  (X.shape[0] +1) * 4
    W_Q = t.nn.Parameter(t.rand(d_Q, emb_dim))
    W_K = t.nn.Parameter(t.rand(d_K, emb_dim))
    W_V = t.nn.Parameter(t.rand(d_V, emb_dim))
    return W_Q, W_K, W_V

  def QKV_matrices(self, X1, X2, W_Q, W_K, W_V):
    Q = W_Q @ X1.T
    K = W_K @ X2.T
    V = W_V @ X2.T
    return Q, K, V

"""## Multi-head attention"""

import torch as t
import torch.nn as nn
import torch.nn.functional as f

seq_len = 4 # take the habits to set the length to the maximum of the sequnee
batch_size = 1 # it will help for parallel processing, is one for demonstration purposes
input_dim = 512 # dim of each word that goes into the attention unit
d_model = 512 # is the dim of the output of the attention unit for every single word
x = t.randn((batch_size,seq_len,input_dim)) # is just a randomly generated input, bcs the positon for example is not taken into in account
# This x is actually the output of the input and position embeddings

x.shape

"""Mapping the input from input_dim to 3* d_model.

This is done to create the query, key and the value vectors all concatenated and all of them have, all the 8 attention heads which we will split up much later
"""

qkv_layer = nn.Linear(input_dim, 3* d_model) # the 3 indicate the combination of q, k, and v

qkv_layer

qkv = qkv_layer(x)

qkv.shape

num_heads = 8
head_dim = d_model // num_heads # 514 // 8 = 64
qkv = qkv.reshape(batch_size, seq_len, num_heads, 3* head_dim) # we use the reshape to break down the last dimension into a product of the number of heads and three times the

qkv.shape

#to transpose the 2nd and 3rd dimensions so that the number of head and the seq_len  position will be permuted to allow easier parallelization operations on these 2 last dim ie now on 4 and 192
qkv = qkv.permute(0, 2, 1, 3)
qkv.shape

q, k, v = qkv.chunk(3, dim=-1) # to obtain the q,k,v individually by breaking down the last dimension

q.shape, k.shape, v.shape

"""### Self attention for multiple heads
For a single head:

$\text{self attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + \text{mask}\right)V$

$\text{new} V = \text{self attention .}V$
"""

#we have a tensor of 4 dim and we need to specify along which dimensions the transpose should be made
import math
d_k = q.size()[-1] # same as q.shape[3]
scaled = t.matmul(q, k.transpose(-2,-1)) / math.sqrt(d_k)
scaled.shape

"""Notes on mask motivation:


1.   The goal of attention is to gain context from words around it.
2.  During the encoding phase we actually have all words which are passed in parallel simultaneously so we can generate vectors by the context of words that come before it as well as words that come after.

1.   In decoder however, we generate words one at a time. So when generating context we only want to look at words that coe before it, bcos we don't even have the words that come after it





"""

mask = t.full(scaled.size(), float('-inf'))
mask

mask.size()

mas = t.full((2, 3, 4, 4), float('-inf'))
mas

mask = t.triu(mask, diagonal=1)
mask

mask = t.full(scaled.size(), float('-inf'))
mask = t.triu(mask, diagonal=1)
''' this convert the mask to upper tringular tensore by keeping above the diagonal the same as they are
 and setting below the diagonal to zeros '''
mask[0,1] # mask for input  to a single and first head

mask[0,2] # this show input to the second head

"""### Function"""

import math
def scaledDotPro(q, k, v, Mask=None):
  d_k = q.size()[-1] # same as q.shape[3]
  scaled = t.matmul(q, k.transpose(-1,-2)) / math.sqrt(d_k)
  if mask is not None:
    scaled += Mask
  attention = f.softmax(scaled, dim=-1)
  values = t.matmul(attention, v)
  return values, attention

values, attention = scaledDotPro(q, k, v, Mask=mask)
values.shape, attention.shape

attention[0][0]

values.size() # this for each single head and each single word

"""Now combine or concatenate all of those heads together meaning to get back a representating vector of the input of 512 dim"""

values = values.reshape(batch_size, seq_len, num_heads * head_dim)
values.size()

"""So that, these heads communicate to each other the information they've learned, we are just going to pass it through a linear layer, which is just a **feed_forward layer** of 512 x 512"""

lin_layer = nn.Linear(d_model, d_model)
lin_layer

out = lin_layer(values)
out

out.shape

"""So this output is now going to be much more context aware than the input vector was.

Class Multi-head attention

Notes on the inputs and outputs


*   The inputs and outputs are given and are out simultaneously through the encoder
*   The outputs are words pieces like byte pair encodings in some language models

*   Pass to the decoder, the output or the translated word will be generate one at the time
*   List item
"""

class MultiheadAttention(nn.Module):
    import torch
    import torch.nn as nn
    import torch.nn.functional as f
    import math

    def __init__(self, input_dim, d_model, num_heads):
        super().__init__()
        self.input_dim = input_dim
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.qkv_layer = nn.Linear(input_dim , 3 * d_model)
        self.linear_layer = nn.Linear(d_model, d_model)


    def scaledDotPro(q, k, v, Mask=None):
      d_k = q.size()[-1] # same as q.shape[3]
      scaled = t.matmul(q, k.transpose(-1,-2)) / math.sqrt(d_k) # the order of -2 and -1 does not matter, just focus on the dimenions?
      if mask is not None:
        scaled += Mask
      attention = f.softmax(scaled, dim=-1)
      values = t.matmul(attention, v)
      return values, attention

      def forward(self, x, mask=None):
        batch_size, sequence_length, input_dim = x.size()
        print(f"x.size(): {x.size()}")
        qkv = self.qkv_layer(x)
        print(f"qkv.size(): {qkv.size()}")
        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)
        print(f"qkv.size(): {qkv.size()}")
        qkv = qkv.permute(0, 2, 1, 3)
        print(f"qkv.size(): {qkv.size()}")
        q, k, v = qkv.chunk(3, dim=-1)
        print(f"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, ")
        values, attention = scaled_dot_product(q, k, v, mask)
        print(f"values.size(): {values.size()}, attention.size:{ attention.size()} ")
        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)
        print(f"values.size(): {values.size()}")
        out = self.linear_layer(values)
        print(f"out.size(): {out.size()}")
        return out

"""## Positional Encoding

## Layer normalisation
"""